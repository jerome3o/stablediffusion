{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FCMR8tDQRf6"
   },
   "source": [
    "Note: There might be a handful of bugs at the moment. The developers of this stable diffusion implementation keep changing the api. Everyone should know not to make breaking api changes so regularly! I'll do a pass over the code and fix bugs as soon as I can. Am away this week :) thanks to Michael d for bringing this to my attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJWFTB8aVxy-"
   },
   "source": [
    "# Stable Diffusion Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cbmk3JqVs5yQ"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers diffusers lpips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scratch/token.txt\") as f:\n",
    "    token = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RtV9OsG2UkUD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, List\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "from torchvision import transforms as tfms\n",
    "import numpy as np\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torchvision.transforms.ToTensor\n",
    "to_tensor_tfm = tfms.ToTensor()\n",
    "\n",
    "def pil_to_latent(input_im):\n",
    "  # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "  with torch.no_grad():\n",
    "    latent = vae.encode(to_tensor_tfm(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
    "  return 0.18215 * latent.mode() # or .mean or .sample\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    \n",
    "  # batch of latents -> list of images\n",
    "  latents = (1 / 0.18215) * latents\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    image = vae.decode(latents)[0]\n",
    "    \n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  images = (image * 255).round().astype(\"uint8\")\n",
    "  pil_images = [Image.fromarray(image) for image in images]\n",
    "  return pil_images\n",
    "\n",
    "# potentially list of str needed?\n",
    "\n",
    "def prep_text(\n",
    "    prompt: str,\n",
    "    tokenizer: Any,\n",
    "    max_length: int = None\n",
    "):\n",
    "    max_length = max_length or tokenizer.model_max_length\n",
    "    \n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    \n",
    "    # Prep text \n",
    "    text_input = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        max_length=tokenizer.model_max_length, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "      text_embeddings = text_encoder(\n",
    "          text_input.input_ids.to(torch_device)\n",
    "      )[0]\n",
    "\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    return text_embeddings, max_length\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    text_embeddings, # tensor?\n",
    "    loading_bar=True,\n",
    "    batch_size=1,\n",
    "    generator=None,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,    \n",
    "    height=512,\n",
    "    width=768,\n",
    "    latents=None\n",
    "    \n",
    "):\n",
    "    if generator is None:\n",
    "        generator = torch.manual_seed(42)\n",
    "    \n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "        \n",
    "    # Prep latents\n",
    "    if latents is None:\n",
    "        latents = torch.randn(\n",
    "            (\n",
    "                batch_size, \n",
    "                unet.in_channels, \n",
    "                height // 8, \n",
    "                width // 8\n",
    "            ),\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    latents = latents.to(torch_device)\n",
    "    latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
    "    \n",
    "    loading_bar_function = tqdm if loading_bar else lambda x: x\n",
    "\n",
    "    # Loop\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in loading_bar_function(enumerate(scheduler.timesteps)):\n",
    "\n",
    "            # expand the latents if we are doing classifier-free guidance \n",
    "            # to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            sigma = scheduler.sigmas[i]\n",
    "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(\n",
    "                    latent_model_input, \n",
    "                    t, \n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                )[\"sample\"]\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "\n",
    "    return latents_to_pil(latents)[0]\n",
    "\n",
    "def get_output_dir(base: Path):\n",
    "    base.mkdir(exist_ok=True, parents=True)\n",
    "    index = max(map(lambda p: int(p.stem), base.glob(\"[0-9]*\")), default=0) + 1\n",
    "    interp_output = base / str(index)\n",
    "    interp_output.mkdir()\n",
    "    return interp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'dual_cross_attention': False, 'use_linear_projection': True} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-7-c5d8707a32b7&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">451</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">448 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> low_cpu_mem_usage:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">449 │   │   │   # Instantiate model with empty weights</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">450 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> accelerate.init_empty_weights():                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>451 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>model, unused_kwargs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.from_config(                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">452 │   │   │   │   │   </span>config_path,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">453 │   │   │   │   │   </span>cache_dir=cache_dir,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">454 │   │   │   │   │   </span>return_unused_kwargs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_config</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   │   │   </span>init_dict[<span style=\"color: #808000; text-decoration-color: #808000\">\"dtype\"</span>] = unused_kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"dtype\"</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 │   │   # Return model and optionally state and/or unused_kwargs</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>174 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(**init_dict)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   </span>return_tuple = (model,)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177 │   │   # Flax schedulers have a state, so return it.</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">inner_init</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">441 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">inner_init</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 │   │   # Ignore private kwargs in the init.</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 │   │   </span>init_kwargs = {k: v <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> kwargs.items() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> k.startswith(<span style=\"color: #808000; text-decoration-color: #808000\">\"_\"</span>)}           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>444 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>init(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **init_kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">445 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, ConfigMixin):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">446 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">447 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"`@register_for_config` was applied to {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span><span style=\"color: #808000; text-decoration-color: #808000\">} init m</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">unet_2d_condition.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">135</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132 │   │   │   </span>output_channel = block_out_channels[i]                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 │   │   │   </span>is_final_block = i == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(block_out_channels) - <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>135 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>down_block = get_down_block(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 │   │   │   │   </span>down_block_type,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   │   │   </span>num_layers=layers_per_block,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   │   │   </span>in_channels=input_channel,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">unet_2d_blocks.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_down_block</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  62 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> down_block_type == <span style=\"color: #808000; text-decoration-color: #808000\">\"CrossAttnDownBlock2D\"</span>:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  63 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> cross_attention_dim <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  64 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"cross_attention_dim must be specified for CrossAttnDownBlo</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  65 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> CrossAttnDownBlock2D(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  66 │   │   │   </span>num_layers=num_layers,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  67 │   │   │   </span>in_channels=in_channels,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  68 │   │   │   </span>out_channels=out_channels,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">unet_2d_blocks.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">535</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 532 │   │   │   </span>attentions.append(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 533 │   │   │   │   </span>Transformer2DModel(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 534 │   │   │   │   │   </span>attn_num_head_channels,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 535 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>out_channels // attn_num_head_channels,                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 536 │   │   │   │   │   </span>in_channels=out_channels,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 537 │   │   │   │   │   </span>num_layers=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 538 │   │   │   │   │   </span>cross_attention_dim=cross_attention_dim,                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>unsupported operand <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">type</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> for <span style=\"color: #800080; text-decoration-color: #800080\">//</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'int'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'list'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<ipython-input-7-c5d8707a32b7>\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m451\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m low_cpu_mem_usage:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m449 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Instantiate model with empty weights\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m450 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m accelerate.init_empty_weights():                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m451 \u001b[2m│   │   │   │   \u001b[0mmodel, unused_kwargs = \u001b[96mcls\u001b[0m.from_config(                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m452 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_path,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m453 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcache_dir=cache_dir,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m454 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mreturn_unused_kwargs=\u001b[94mTrue\u001b[0m,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m17\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m4\u001b[0m in \u001b[92mfrom_config\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_dict[\u001b[33m\"\u001b[0m\u001b[33mdtype\u001b[0m\u001b[33m\"\u001b[0m] = unused_kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mdtype\u001b[0m\u001b[33m\"\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Return model and optionally state and/or unused_kwargs\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m174 \u001b[2m│   │   \u001b[0mmodel = \u001b[96mcls\u001b[0m(**init_dict)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_tuple = (model,)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Flax schedulers have a state, so return it.\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m44\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m4\u001b[0m in \u001b[92minner_init\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m441 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92minner_init\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Ignore private kwargs in the init.\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   \u001b[0minit_kwargs = {k: v \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m kwargs.items() \u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m k.startswith(\u001b[33m\"\u001b[0m\u001b[33m_\u001b[0m\u001b[33m\"\u001b[0m)}           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m444 \u001b[2m│   │   \u001b[0minit(\u001b[96mself\u001b[0m, *args, **init_kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m445 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(\u001b[96mself\u001b[0m, ConfigMixin):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m446 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m`@register_for_config` was applied to \u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m.\u001b[91m__name__\u001b[0m\u001b[33m}\u001b[0m\u001b[33m init m\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/\u001b[0m\u001b[1;33munet_2d_condition.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m135\u001b[0m in \u001b[92m__init__\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput_channel = block_out_channels[i]                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   │   \u001b[0mis_final_block = i == \u001b[96mlen\u001b[0m(block_out_channels) - \u001b[94m1\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m135 \u001b[2m│   │   │   \u001b[0mdown_block = get_down_block(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdown_block_type,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mnum_layers=layers_per_block,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   │   \u001b[0min_channels=input_channel,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/\u001b[0m\u001b[1;33munet_2d_blocks.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m65\u001b[0m in \u001b[92mget_down_block\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  62 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m down_block_type == \u001b[33m\"\u001b[0m\u001b[33mCrossAttnDownBlock2D\u001b[0m\u001b[33m\"\u001b[0m:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  63 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m cross_attention_dim \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  64 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mcross_attention_dim must be specified for CrossAttnDownBlo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  65 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m CrossAttnDownBlock2D(                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  66 \u001b[0m\u001b[2m│   │   │   \u001b[0mnum_layers=num_layers,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  67 \u001b[0m\u001b[2m│   │   │   \u001b[0min_channels=in_channels,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  68 \u001b[0m\u001b[2m│   │   │   \u001b[0mout_channels=out_channels,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jerome/miniconda3/envs/ldm/lib/python3.8/site-packages/diffusers/models/\u001b[0m\u001b[1;33munet_2d_blocks.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m535\u001b[0m in \u001b[92m__init__\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 532 \u001b[0m\u001b[2m│   │   │   \u001b[0mattentions.append(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 533 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mTransformer2DModel(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 534 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattn_num_head_channels,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 535 \u001b[2m│   │   │   │   │   \u001b[0mout_channels // attn_num_head_channels,                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 536 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0min_channels=out_channels,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 537 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mnum_layers=\u001b[94m1\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 538 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcross_attention_dim=cross_attention_dim,                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0munsupported operand \u001b[1;35mtype\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m for \u001b[35m/\u001b[0m\u001b[35m/\u001b[0m: \u001b[32m'int'\u001b[0m and \u001b[32m'list'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t96p567IUqzB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQM7XgxYV6hg"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYoviVgUXjwZ"
   },
   "outputs": [],
   "source": [
    "output_dir = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntjZ-QbCxVSA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    \"Fjords\"\n",
    "]\n",
    "height = 512\n",
    "width = 768\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "text_embeddings, max_length = prep_text(prompt, tokenizer)\n",
    "uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "for i in range(3):\n",
    "    display(generate_image(\n",
    "        text_embeddings,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.manual_seed(i+1),\n",
    "        batch_size=batch_size,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp\")\n",
    "# current_output_dir = Path(output_dir) / \"interp\" / \"4\"\n",
    "\n",
    "\n",
    "prompt_1 = [\n",
    "    \"ancient village with small huts, two men with spears talking\"\n",
    "]\n",
    "prompt_2 = [\n",
    "    \"futuristic city, flying cars, city lights, busy street, 4k, photograph of the year\"\n",
    "]\n",
    "height = 512\n",
    "width = 768\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "\n",
    "images = []\n",
    "\n",
    "\n",
    "text_embeddings_1, max_length = prep_text(prompt_1, tokenizer)\n",
    "text_embeddings_2, max_length = prep_text(prompt_2, tokenizer)\n",
    "\n",
    "delta = text_embeddings_2 - text_embeddings_1\n",
    "\n",
    "for i, fraction in tqdm(enumerate(np.linspace(0, 1))):\n",
    "\n",
    "    text_embeddings_combined = text_embeddings_1 + delta*fraction\n",
    "\n",
    "    uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings_combined])\n",
    "\n",
    "    im = generate_image(\n",
    "        text_embeddings,\n",
    "        loading_bar=False,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.manual_seed(42),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    images.append(im)\n",
    "    im.save(current_output_dir / f\"image_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMcYtRltEmzb"
   },
   "source": [
    "## N step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepConfig(BaseModel):\n",
    "    prompts: List[str]\n",
    "    n_interp_steps: int = 60\n",
    "        \n",
    "class _Step(BaseModel):\n",
    "    origin_image_prompt: str\n",
    "    origin_image_index: int\n",
    "    target_image_prompt: str\n",
    "    target_image_index: int\n",
    "    file_name: str\n",
    "    fraction: float\n",
    "        \n",
    "class NStepSnapshotConfig(BaseModel):\n",
    "    config: NStepConfig\n",
    "    steps: List[_Step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etHajiDAUyMO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_n_steps\")\n",
    "\n",
    "factor = 1\n",
    "height = int(512*factor)\n",
    "width = int(768*factor)\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "get_generator = lambda: torch.manual_seed(43)\n",
    "batch_size = 1\n",
    "\n",
    "config = NStepConfig(\n",
    "    n_interp_steps=60,\n",
    "    prompts=[\n",
    "        \"Vladimir Putin\",\n",
    "        \"Donald Trump\",\n",
    "        \"A baby in a suit\",\n",
    "        \"Vladimir Putin\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "embeddings = [\n",
    "    prep_text([prompt], tokenizer)[0]\n",
    "    for prompt in config.prompts\n",
    "]\n",
    "\n",
    "deltas = [\n",
    "    embeddings[i+1] - embeddings[i]\n",
    "    for i in range(len(embeddings) - 1)\n",
    "]\n",
    "\n",
    "# scout\n",
    "print(\"scouting\")\n",
    "\n",
    "for i, e in enumerate(embeddings):\n",
    "    (current_output_dir / \"scout\").mkdir(exist_ok=True)\n",
    "    \n",
    "    uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "    e = torch.cat([uncond_embeddings, e])\n",
    "    \n",
    "    generate_image(\n",
    "        e,\n",
    "        loading_bar=True,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=get_generator(),\n",
    "        batch_size=batch_size,\n",
    "    ).save(current_output_dir / \"scout\" / f\"{i}.png\")\n",
    "\n",
    "print(\"scouting - complete\")\n",
    "\n",
    "tmp = [\n",
    "    (i, frac )\n",
    "    for i in range(len(config.prompts) - 1)\n",
    "    for frac in np.linspace(0, 1, config.n_interp_steps)\n",
    "]\n",
    "\n",
    "steps = [\n",
    "    _Step(\n",
    "        origin_image_index=i,\n",
    "        origin_image_prompt=config.prompts[i],\n",
    "        target_image_index=i+1,\n",
    "        target_image_prompt=config.prompts[i+1],\n",
    "        fraction=frac,\n",
    "        file_name=f\"image_{c:05d}_{i+frac:.5}.png\",\n",
    "    )\n",
    "    for c, (i, frac) in enumerate(tmp)\n",
    "]\n",
    "\n",
    "snapshot_config = NStepSnapshotConfig(config=config, steps=steps)\n",
    "\n",
    "with open(current_output_dir / \"config.json\", \"w\") as f:\n",
    "    f.write(snapshot_config.json(indent=4))\n",
    "\n",
    "    \n",
    "for step in tqdm(steps):\n",
    "    fn = current_output_dir / step.file_name\n",
    "    if fn.exists():\n",
    "        continue\n",
    "    \n",
    "    text_embeddings_1 = embeddings[step.origin_image_index]\n",
    "    delta = deltas[step.origin_image_index]\n",
    "    text_embeddings_combined = text_embeddings_1 + delta*step.fraction\n",
    "\n",
    "    uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings_combined])\n",
    "\n",
    "    im = generate_image(\n",
    "        text_embeddings,\n",
    "        loading_bar=False,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=get_generator(),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    im.save(current_output_dir / step.file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_variable_step\")\n",
    "\n",
    "\n",
    "prompt_1 = [\n",
    "    \"ancient village with small huts, two men with spears talking\"\n",
    "]\n",
    "prompt_2 = [\n",
    "    \"futuristic city, flying cars, city lights, busy street, 4k, photograph of the year\"\n",
    "]\n",
    "height = 512 // 2\n",
    "width = 768 // 2\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "\n",
    "images = []\n",
    "\n",
    "\n",
    "text_embeddings_1, max_length = prep_text(prompt_1, tokenizer)\n",
    "text_embeddings_2, max_length = prep_text(prompt_2, tokenizer)\n",
    "\n",
    "delta = text_embeddings_2 - text_embeddings_1\n",
    "\n",
    "for i, fraction in tqdm(enumerate(np.linspace(0, 1))):\n",
    "\n",
    "    text_embeddings_combined = text_embeddings_1 + delta*fraction\n",
    "\n",
    "    uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings_combined])\n",
    "\n",
    "    current_im = generate_image(\n",
    "        text_embeddings,\n",
    "        loading_bar=False,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.manual_seed(42),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    current_im.save(interp_output / f\"image_{i}.png\")\n",
    "    \n",
    "    if i>2:\n",
    "        break\n",
    "    \n",
    "    prev_im = current_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU-g0cqfWHGp"
   },
   "outputs": [],
   "source": [
    "current_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "diff = ImageChops.difference(prev_im, current_im)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = diff.getdata()\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Latent Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent\")\n",
    "# current_output_dir = Path(output_dir) / \"interp_latent\" / \"1\"\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    \"A litter of shih tzu puppies\"\n",
    "]\n",
    "height = 512\n",
    "width = 768\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "n_points_interp = 180\n",
    "\n",
    "latents_list = [\n",
    "    torch.randn(\n",
    "        (\n",
    "            batch_size, \n",
    "            unet.in_channels, \n",
    "            height // 8, \n",
    "            width // 8\n",
    "        ),\n",
    "        generator=torch.manual_seed(i),\n",
    "    )\n",
    "    for i in [42, 2, 3, 4]\n",
    "]\n",
    "latents_list += [latents_list[0]]\n",
    "\n",
    "steps = [\n",
    "    (i, frac)\n",
    "    for i in range(len(latents_list) - 1)\n",
    "    for frac in np.linspace(0, 1, n_points_interp)\n",
    "]\n",
    "\n",
    "text_embeddings_orig, max_length = prep_text(prompt, tokenizer)\n",
    "uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings_orig])\n",
    "\n",
    "\n",
    "for c, (i, fraction) in tqdm(list(enumerate(steps))):\n",
    "    im_file = current_output_dir / f\"image_{c:05d}.png\"\n",
    "    if im_file.exists():\n",
    "        continue\n",
    "    \n",
    "    latents_1 = latents_list[i]\n",
    "    latents_2 = latents_list[i+1]\n",
    "    \n",
    "    delta = latents_2 - latents_1\n",
    "    latents = latents_1 + delta*fraction\n",
    "    latents -= latents.mean()\n",
    "    latents /= latents.std()\n",
    "    im = generate_image(\n",
    "        text_embeddings,\n",
    "        latents=latents,\n",
    "        loading_bar=False,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    im.save(im_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept + Latent interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    prompt_list: List[str]\n",
    "    \n",
    "        \n",
    "    n_steps_latent: int = 60\n",
    "    n_steps_prompt: int = 360\n",
    "\n",
    "\n",
    "class _Step(BaseModel):\n",
    "    # (Prompt, Weight)\n",
    "    # [(\"prompt_1\", 0.5), ...]\n",
    "    prompts: List[Tuple[str, float]]\n",
    "        \n",
    "    # (Seed integer, Weight)\n",
    "    latents: List[Tuple[int, float]]\n",
    "        \n",
    "        \n",
    "def generate_steps(config: Config):\n",
    "    latent_fracs = np.linspace(0, 1, config.n_steps_latent)\n",
    "    prompt_fracs = np.linspace(0, 1, config.n_steps_prompt)\n",
    "    \n",
    "    c = 0\n",
    "    steps = []\n",
    "    \n",
    "    for prompt_i, prompt in enumerate(config.prompt_list[:-1]):\n",
    "        prompt_2 = config.prompt_list[prompt_i + 1]\n",
    "        for prompt_frac in prompt_fracs:\n",
    "            \n",
    "            # calc latent seeds\n",
    "            latent_frac = latent_fracs[c % config.n_steps_latent]\n",
    "            seed_1 = c // config.n_steps_latent\n",
    "            seed_2 = seed_1 + 1\n",
    "            \n",
    "            steps.append(\n",
    "                _Step(\n",
    "                    prompts=[(prompt, 1 - prompt_frac), (prompt_2, prompt_frac)],\n",
    "                    latents=[(seed_1, 1 - latent_frac), (seed_2, latent_frac)],\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            c += 1\n",
    "    return steps\n",
    "\n",
    "\n",
    "def generate_latent(latent_spec: List[Tuple[int, float]]) -> torch.Tensor:\n",
    "    latent = sum([\n",
    "        frac * torch.randn(\n",
    "            (\n",
    "                batch_size, \n",
    "                unet.in_channels, \n",
    "                height // 8, \n",
    "                width // 8\n",
    "            ),\n",
    "            generator=torch.manual_seed(seed),\n",
    "        )\n",
    "        for seed, frac in latent_spec\n",
    "    ])\n",
    "    latent -= latent.mean()\n",
    "    latent /= latent.std()\n",
    "    return latent\n",
    "            \n",
    "\n",
    "\n",
    "def generate_images(\n",
    "    steps: List[_Step], \n",
    "    output_dir: str,\n",
    "    n_scout_images: int = 20\n",
    "):\n",
    "    \n",
    "    uncond_embeddings, _ = prep_text([\"\"], tokenizer)\n",
    "    \n",
    "    all_prompts = list({prompt for step in steps for prompt, _ in step.prompts})\n",
    "    embeddings = {\n",
    "        prompt: prep_text([prompt], tokenizer)[0]\n",
    "        for prompt in list(set(all_prompts))\n",
    "    }\n",
    "    \n",
    "    enumerated_steps = list(enumerate(steps))\n",
    "    scout_step = len(enumerated_steps) // n_scout_images\n",
    "    \n",
    "    print(scout_step)\n",
    "    print(len(enumerated_steps[::scout_step]))\n",
    "    print(len([s for s in enumerated_steps if (s[0] % scout_step) != 0]))\n",
    "    ordered_steps = enumerated_steps[::scout_step] + [s for s in enumerated_steps if (s[0] % scout_step) != 0]\n",
    "    \n",
    "    for c, step in tqdm(ordered_steps):\n",
    "        \n",
    "        im_file = output_dir / f\"image_{c:05d}.png\"\n",
    "        \n",
    "        if im_file.exists():\n",
    "            continue\n",
    "\n",
    "        latent = generate_latent(step.latents)\n",
    "        \n",
    "        embedding = sum([\n",
    "            embeddings[prompt] * frac\n",
    "            for prompt, frac in step.prompts\n",
    "        ])\n",
    "        embedding = torch.cat([uncond_embeddings, embedding])\n",
    "\n",
    "        im = generate_image(\n",
    "            embedding,\n",
    "            latents=latent,\n",
    "            loading_bar=False,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        im.save(im_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\")\n",
    "\n",
    "height = 512\n",
    "width = 768\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "\n",
    "prompt_list = [\n",
    "    \"An octopus, national geographic, 4k, high resolution\",\n",
    "    \"A robotic octopus, steampunk\",\n",
    "    \"A messy pile of wires, tangled, mess\",\n",
    "    \"A bowl of spaghetti, overflowing\",\n",
    "    \"Flying spaghetti monster in the sky\",\n",
    "    \"Flying spaghetti monster, ancient drawing, hieroglyphics, worshipers\",\n",
    "]\n",
    "\n",
    "\n",
    "_steps_factor = 5\n",
    "n_steps_latent = 60 * _steps_factor\n",
    "n_steps_prompt = 360 * _steps_factor\n",
    "    \n",
    "config = Config(\n",
    "    prompt_list=prompt_list\n",
    ")\n",
    "\n",
    "steps = generate_steps(config)\n",
    "\n",
    "generate_images(steps, current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sin_wave_embeddings(\n",
    "    prompt_1: str, \n",
    "    prompt_2: str, \n",
    "    n_periods: int = 5,\n",
    "    points_per_period: int = 360,\n",
    "    points_per_seed: int = 180,\n",
    "    amplitude: float = 0.2,\n",
    "    mean: float = 0.5,\n",
    "):\n",
    "    prompt_fracs = np.sin(np.linspace(\n",
    "        0, \n",
    "        2*np.pi*n_periods, \n",
    "        points_per_period*n_periods\n",
    "    )) * amplitude + mean\n",
    "    \n",
    "    latent_fracs = np.linspace(0, 1, points_per_seed)\n",
    "    \n",
    "    steps = []\n",
    "    \n",
    "    for i, prompt_frac in enumerate(prompt_fracs):\n",
    "\n",
    "        # calc latent seeds\n",
    "        latent_frac = latent_fracs[c % config.n_steps_latent]\n",
    "        seed_1 = i // points_per_seed\n",
    "        seed_2 = seed_1 + 1\n",
    "        \n",
    "        \n",
    "        steps.append(\n",
    "            _Step(\n",
    "                prompts=[(prompt_1, 1 - prompt_frac), (prompt_2, prompt_frac)],\n",
    "                latents=[(seed_1, 1 - latent_frac), (seed_2, latent_frac)],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return steps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = generate_sin_wave_embeddings(\n",
    "    \"A frog catching a fly\",\n",
    "    \"A swan swimming in a lake\",\n",
    ")\n",
    "generate_images(\n",
    "    steps,\n",
    "    get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\")\n",
    "\n",
    "steps = generate_sin_wave_embeddings(\n",
    "    \"A swan swimming in a lake\",\n",
    "    \"A horse grazing in a field of flowers\",\n",
    "    amplitude=0.1,\n",
    "    points_per_seed=100,\n",
    "    n_periods=10,\n",
    ")\n",
    "\n",
    "generate_images(\n",
    "    steps,\n",
    "    current_output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overshoot_steps(\n",
    "    prompt_1: str,\n",
    "    prompt_2: str,\n",
    "    n_points: int = 1000,\n",
    "    points_per_seed: int = 180,\n",
    "):\n",
    "    half_cycle = np.linspace(0, np.pi, n_points // 2)\n",
    "    frac_list = np.concatenate([np.cos(half_cycle) / 2 + 1, np.cos(half_cycle) / 2])\n",
    "    latent_fracs = np.linspace(0, 1, points_per_seed)\n",
    "    \n",
    "    steps = []\n",
    "    for i, frac in enumerate(frac_list):\n",
    "        \n",
    "        # calc latent seeds\n",
    "        latent_frac = latent_fracs[c % config.n_steps_latent]\n",
    "        seed_1 = i // points_per_seed\n",
    "        seed_2 = seed_1 + 1\n",
    "        \n",
    "        steps.append(_Step(\n",
    "            prompts=[\n",
    "                (prompt_1, frac),\n",
    "                (prompt_2, 1 - frac),\n",
    "            ],\n",
    "            latents=[\n",
    "                (seed_1, 1 - latent_frac),\n",
    "                (seed_2, latent_frac),\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\")\n",
    "\n",
    "steps = generate_overshoot_steps(\n",
    "    \"A watercolor painting of a bookshelf\",\n",
    "    \"A pencil schematic of a complex engine\",\n",
    "    n_points=2000,\n",
    "    points_per_seed=100\n",
    ")\n",
    "\n",
    "generate_images(\n",
    "    steps,\n",
    "    current_output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\")\n",
    "\n",
    "height = 512\n",
    "width = 768\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "generator = torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "\n",
    "prompt_list = [\n",
    "    \"A giant bird flying above an ocean\",\n",
    "    \"A giant steampunk bird, flying above an industrial zone, emitting steam\",\n",
    "    \"A giant robotic pterodactyl\",\n",
    "    \"A pterodactyl flying above a pre-historic landscape\",\n",
    "    \"A flock of pterodactyls standing around some food\",\n",
    "    \"A giant meteor entering the atmostphere above a landscape of dinosaurs\",\n",
    "    \"A world covered in smoke and ash\",\n",
    "    \"The fossil of a pterodactyl, dig site, archaeology\",\n",
    "]\n",
    "\n",
    "\n",
    "_steps_factor = 5\n",
    "n_steps_latent = 60 * _steps_factor\n",
    "n_steps_prompt = 360 * _steps_factor\n",
    "    \n",
    "config = Config(\n",
    "    prompt_list=prompt_list\n",
    ")\n",
    "\n",
    "steps = generate_steps(config)\n",
    "\n",
    "generate_images(steps, current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_output_dir = get_output_dir(Path(output_dir) / \"interp_latent_and_embeddings\")\n",
    "\n",
    "prompt_list = [\n",
    "    \"Milford sound\",\n",
    "    \"Beautiful fjord\",\n",
    "    \"Beautiful fjord with a wooden cabin\",\n",
    "    \"Clouds rolling over a fjord with a wooden cabin\",\n",
    "    \"Rain clouds above a fjord with a wooden cabin\",\n",
    "    \"A wooden cabin in a fjord, torrential rain\",\n",
    "    \"Looking through the window of a cabin, people around the fireplace, raining outside\",\n",
    "    \"a game of cards around the fireplace, wooden cabin\",\n",
    "    \"A gentle fire in the fireplace of a wooden cabin\",\n",
    "    \"Embers in the fireplace of a wooden cabin\"\n",
    "]\n",
    "\n",
    "\n",
    "_steps_factor = 7\n",
    "n_steps_latent = 180 * _steps_factor\n",
    "n_steps_prompt = 360 * _steps_factor\n",
    "    \n",
    "config = Config(\n",
    "    prompt_list=prompt_list,\n",
    "    n_steps_latent=n_steps_latent,\n",
    "    n_steps_prompt=n_steps_prompt,\n",
    ")\n",
    "\n",
    "steps = generate_steps(config)\n",
    "\n",
    "generate_images(\n",
    "    steps,\n",
    "    current_output_dir,\n",
    "    n_scout_images=len(prompt_list)**3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITl1oLuxWDGf"
   },
   "source": [
    "# Scheduling and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Lb7v2EfICDA"
   },
   "outputs": [],
   "source": [
    "im = Image.open('/content/drive/MyDrive/stablediff/shitake3.png').convert('RGB')\n",
    "im = im.resize((512,512))\n",
    "encoded = pil_to_latent(im)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ve_2EROK1Jd"
   },
   "source": [
    "# Image to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JpjEKYlXXFd0"
   },
   "outputs": [],
   "source": [
    "# Make a folder to store results\n",
    "!rm -rf /content/drive/MyDrive/stablediff/noise2\n",
    "!mkdir -p /content/drive/MyDrive/stablediff/noise2\n",
    "\n",
    "# View a noised version\n",
    "noise = torch.randn_like(encoded) # Random noise\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    timestep = i * 5\n",
    "    encoded_and_noised = scheduler.add_noise(encoded, noise, timestep)\n",
    "    img = latents_to_pil(encoded_and_noised)[0]\n",
    "    img.save(f'/content/drive/MyDrive/stablediff/noise/{i:04}.jpeg')\n",
    "\n",
    "!ffmpeg -v 1 -y -f image2 -framerate 12 -i /content/drive/MyDrive/stablediff/noise/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n",
    "mp4 = open('out.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=512 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCIn_9XnXzAr"
   },
   "outputs": [],
   "source": [
    "prompt = [\"wooden carving of a rabbit eating a leaf, highly detailed, 4k, artisan\"]\n",
    "height = 512                       \n",
    "width = 512                        \n",
    "num_inference_steps = 50   \n",
    "guidance_scale = 8            \n",
    "generator = torch.manual_seed(1)   \n",
    "batch_size = 1\n",
    "\n",
    "# Prep text \n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "# Prep Scheduler\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Start step\n",
    "start_step = 25\n",
    "start_sigma = scheduler.sigmas[start_step]\n",
    "start_timestep = int(scheduler.timesteps[start_step])\n",
    "\n",
    "# Prep latents\n",
    "noise = torch.randn_like(encoded)\n",
    "latents = scheduler.add_noise(encoded, noise, start_timestep)\n",
    "latents = latents.to(torch_device)\n",
    "latents = latents * start_sigma  # << NB\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):\n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    if i > start_step:\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "      sigma = scheduler.sigmas[i]\n",
    "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "\n",
    "latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYwsF4udLXY_"
   },
   "source": [
    "# Mixed guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB3o7-WZGo5l"
   },
   "outputs": [],
   "source": [
    "prompts = ['blue fire', 'reticulated python in a tree']\n",
    "weights = [0.5,0.5]\n",
    "height = 512                \n",
    "width = 768                      \n",
    "num_inference_steps = 50 \n",
    "guidance_scale = 8 \n",
    "generator = torch.manual_seed(5)\n",
    "batch_size = 1\n",
    "\n",
    "# Prep text \n",
    "# Embed both prompts\n",
    "text_embeddings = []\n",
    "for i in range(len(prompts)):\n",
    "    text_input = tokenizer([prompts[i]], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings.append(text_encoder(text_input.input_ids.to(torch_device))[0])\n",
    "\n",
    "# Take the average\n",
    "weighted_embeddings = torch.zeros(text_embeddings[0].shape).to(torch_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(prompts)):\n",
    "        weighted_embeddings.add_(text_embeddings[i] * weights[i])\n",
    "\n",
    "text_embeddings = weighted_embeddings\n",
    "\n",
    "# And the uncond. input as before:\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "# Prep Scheduler\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "  generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)\n",
    "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):\n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    sigma = scheduler.sigmas[i]\n",
    "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "\n",
    "latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV4JlBIZqxvc"
   },
   "source": [
    "# Denoising Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By-8CPHZM_v8"
   },
   "outputs": [],
   "source": [
    "prompt = 'a futuristic city, abandoned and overgrown with plants, dystopia, bathed in sunlight'\n",
    "height = 512                     \n",
    "width = 768 \n",
    "num_inference_steps = 100 \n",
    "guidance_scale = 8     \n",
    "generator = torch.manual_seed(10) \n",
    "batch_size = 1\n",
    "\n",
    "# Make a folder to store results\n",
    "!rm -rf /content/drive/MyDrive/stablediff/denoising2/\n",
    "!mkdir -p /content/drive/MyDrive/stablediff/denoising2/\n",
    "\n",
    "# Prep text \n",
    "text_input = tokenizer([prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "# And the uncond. input as before:\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "# Prep Scheduler\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "  generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)\n",
    "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):\n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    \n",
    "    im_input = latents_to_pil(latents)[0]\n",
    "\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    sigma = scheduler.sigmas[i]\n",
    "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    \n",
    "    # Get the predicted x0:\n",
    "    latents_x0 = latents - sigma * noise_pred\n",
    "\n",
    "    im_t0 = latents_to_pil(latents_x0)[0]\n",
    "    im_noise = latents_to_pil(sigma * noise_pred)[0]\n",
    "\n",
    "    # And the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "    im_next = latents_to_pil(latents)[0]\n",
    "\n",
    "    # Combine the two images and save for later viewing\n",
    "    im = Image.new('RGB', (2304, 512))\n",
    "    im.paste(im_input, (0, 0))\n",
    "    im.paste(im_noise, (768, 0))\n",
    "    im.paste(im_t0, (1536, 0))\n",
    "    im.save(f'/content/drive/MyDrive/stablediff/denoising2/{i:04}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cg41ncctOnpC"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -v 1 -y -f image2 -framerate 12 -i /content/drive/MyDrive/stablediff/denoising2/%04d.jpg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n",
    "mp4 = open('out.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=1536 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "48115d5f033765836d7d3a3d6ec3bb99247d85c0fc4a75ae4218943fd74003ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
